{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "CLINTON    93\n",
       "TRUMP      71\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('discours_US.csv', sep='\\t')\n",
    "df.value_counts('speaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['speaker', 'text', 'date', 'descr', 'link'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Create a Document for each sentence\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         doc \u001b[38;5;241m=\u001b[39m Document(\n\u001b[0;32m     25\u001b[0m             texte\u001b[38;5;241m=\u001b[39msentence, \n\u001b[0;32m     26\u001b[0m             titre\u001b[38;5;241m=\u001b[39mtitle, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m             url\u001b[38;5;241m=\u001b[39msource\n\u001b[0;32m     30\u001b[0m         )\n\u001b[1;32m---> 31\u001b[0m         \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43majouter_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add each document to the corpus\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Now pass the list of documents to Corpus\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(corpus\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m())\n",
      "File \u001b[1;32mc:\\Users\\Belqees\\Desktop\\LYON 2\\M1 Info\\python\\projetpython\\Corpus.py:116\u001b[0m, in \u001b[0;36mCorpus.ajouter_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnaut \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauthors[document\u001b[38;5;241m.\u001b[39mauteur]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndoc, document)\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_vocabulary_and_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Belqees\\Desktop\\LYON 2\\M1 Info\\python\\projetpython\\Corpus.py:128\u001b[0m, in \u001b[0;36mCorpus.update_vocabulary_and_matrices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m build_vocab(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2doc\u001b[38;5;241m.\u001b[39mvalues()))  \u001b[38;5;66;03m# Rebuild vocabulary\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmat_TF \u001b[38;5;241m=\u001b[39m build_term_frequency_matrix(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2doc\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)  \u001b[38;5;66;03m# Rebuild TF matrix\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmat_TFxIDF \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_tfidf_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmat_TF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndoc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Belqees\\Desktop\\LYON 2\\M1 Info\\python\\projetpython\\SearchEngine.py:170\u001b[0m, in \u001b[0;36mcompute_tfidf_matrix\u001b[1;34m(mat_TF, vocab, num_docs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         idf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog((num_docs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (doc_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Adding 1 to avoid zero issues in log\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;66;03m# Multiply the term frequency for each document by the IDF\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         \u001b[43mmat_TFxIDF\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_id\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m mat_TF[:, word_id]\u001b[38;5;241m.\u001b[39mmultiply(idf)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mat_TFxIDF\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\scipy\\sparse\\_csr.py:41\u001b[0m, in \u001b[0;36m_csr_base.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     44\u001b[0m         key \u001b[38;5;241m=\u001b[39m key[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\scipy\\sparse\\_index.py:142\u001b[0m, in \u001b[0;36mIndexMixin.__setitem__\u001b[1;34m(self, key, x)\u001b[0m\n\u001b[0;32m    140\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtocoo(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    141\u001b[0m     x\u001b[38;5;241m.\u001b[39msum_duplicates()\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_arrayXarray_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Make x and i into the same shape\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:917\u001b[0m, in \u001b[0;36m_cs_matrix._set_arrayXarray_sparse\u001b[1;34m(self, row, col, x)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_arrayXarray_sparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, col, x):\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;66;03m# clear entries that will be overwritten\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zero_many\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_swap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m     M, N \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# matches col.shape\u001b[39;00m\n\u001b[0;32m    920\u001b[0m     broadcast_row \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1075\u001b[0m, in \u001b[0;36m_cs_matrix._zero_many\u001b[1;34m(self, i, j)\u001b[0m\n\u001b[0;32m   1073\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[0;32m   1074\u001b[0m offsets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(n_samples, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1075\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mcsr_sample_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# rinse and repeat\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_duplicates()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Author import Author\n",
    "from Document import Document\n",
    "from Corpus import Corpus\n",
    "import re\n",
    "\n",
    "corpus = Corpus.get_instance(df) # Assuming df is already a DataFrame\n",
    "\n",
    "# Create a list to hold the Document instances\n",
    "document_list = []\n",
    "\n",
    "# Ajouter chaque phrase comme document\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    title = row.get('title', f\"Discours {index}\")  # If 'title' column exists\n",
    "    source = row.get('source', 'Unknown')  # If 'source' column exists\n",
    "    date = row.get('date', 'Unknown')  # If 'date' column exists\n",
    "\n",
    "\n",
    "    # Split text into sentences using regular expression\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Adjusted to match end of sentence better\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Create a Document for each sentence\n",
    "        doc = Document(\n",
    "            texte=sentence, \n",
    "            titre=title, \n",
    "            auteur=Author(row['speaker']), \n",
    "            date=date, \n",
    "            url=source\n",
    "        )\n",
    "        corpus.ajouter_document(doc)  # Add each document to the corpus\n",
    "\n",
    "# Now pass the list of documents to Corpus\n",
    "# Display the result\n",
    "print(corpus.__repr__())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation avec widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "label = widgets.Label(value=' Our Search Engine')\n",
    "\n",
    "keywords = widgets.Text(\n",
    "    description='Keywords:',\n",
    "    placeholder='Enter your keywords',\n",
    ")\n",
    "\n",
    "documents_slider = widgets.IntSlider(\n",
    "    value= 5,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Documents:',\n",
    "    continuous_update=False,\n",
    ")\n",
    "display(label, keywords, documents_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import textinput\n",
    "\n",
    "from SearchEngine import *\n",
    "\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Fonction pour lancer la recherche lors du clic sur le bouton\n",
    "def on_button_click(b):\n",
    "    from SearchEngine import build_vocab\n",
    "\n",
    "    with output:\n",
    "        clear_output()\n",
    "        query = textinput.value\n",
    "        num_results = documents_slider.value\n",
    "        results = SearchEngine.search(query)\n",
    "        for doc in results[:num_results]:\n",
    "            print(doc)\n",
    "\n",
    "# Création du bouton\n",
    "button = widgets.Button(description=\"Lancer la recherche\")\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Affichage du bouton et du widget Output\n",
    "display(button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de filtres possibles, par exemple, par auteur\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        query = textinput.value\n",
    "        num_results = documents_slider.value\n",
    "        # Filtrage par auteur (facultatif)\n",
    "        author_filter = 'Obama'  # Exemple d'auteur\n",
    "        filtered_docs = [doc for doc in SearchEngine.search(query) if doc.author == author_filter]\n",
    "        for doc in filtered_docs[:num_results]:\n",
    "            print(doc)\n",
    "\n",
    "# Affichage mis à jour avec le bouton\n",
    "button.on_click(on_button_click)\n",
    "display(button, output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
